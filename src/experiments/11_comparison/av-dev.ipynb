{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import julia\n",
    "julia.install()\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# # Point to the top of the project relative to this script\n",
    "def projectdir(*args):\n",
    "    return str(Path.cwd().joinpath(\"..\", \"..\", \"..\", *args).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from julia import Main as jl\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ipdb\n",
    "import torch\n",
    "# from torchvision.transforms import Lambda\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision import models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "class DDVFAStrategy():\n",
    "    \"\"\"DDVFA Strategy\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        jl.project_dir = projectdir()\n",
    "        jl.eval(\"using Pkg; Pkg.activate(project_dir)\")\n",
    "        jl.eval(\"using AdaptiveResonance\")\n",
    "        jl.eval(\"art = DDVFA(rho_lb=0.4, rho_ub=0.75)\")\n",
    "        # jl.eval(\"art.config = DataConfig(0.0, 255.0, 28*28)\")\n",
    "        jl.eval(\"art.config = DataConfig(-1.0, 1.0, 28*28)\")\n",
    "        \n",
    "        rn = resnet50()\n",
    "        self.mod = create_feature_extractor(rn, {'layer4': 'layer4'})\n",
    "        self.mod = self.mod.to('cuda')\n",
    "        self.mod.eval()\n",
    "        self.weights = ResNet50_Weights.DEFAULT\n",
    "        self.preprocess = self.weights.transforms()\n",
    "        # self.gs = Grayscale(num_output_channels=3)\n",
    "        # self.trans = Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
    "        # self.model = Resnet50Extractor()\n",
    "\n",
    "    def ext_features(self, img):\n",
    "        # prep = preprocess(img).unsqueeze(0)\n",
    "        # print(img.shape)\n",
    "        # rgbimg = self.gs(img)\n",
    "        # rgbimg = Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "        # rgbimg = self.trans(img)\n",
    "        # print(rgbimg.shape)\n",
    "        # prep = self.preprocess(rgbimg)\n",
    "        img = img.to('cuda')\n",
    "        prep = self.preprocess(img)\n",
    "        features = self.mod(prep)['layer4']\n",
    "\n",
    "        avg_features = features.mean(dim=1).flatten().detach().cpu().numpy()\n",
    "        # avg_features.flatten().detach().numpy()\n",
    "        return avg_features\n",
    "\n",
    "    def train(self, experience):\n",
    "        train_dataset = experience.dataset\n",
    "        t = experience.task_label\n",
    "        train_data_loader = DataLoader(\n",
    "            # train_dataset,\n",
    "            dataset=train_dataset,\n",
    "            pin_memory=True,\n",
    "            # num_workers=1,\n",
    "            # num_workers=4,\n",
    "            # batch_size=128,\n",
    "            batch_size=32,\n",
    "            # transform=self.trans\n",
    "        )\n",
    "        print(experience.dataset.__len__())\n",
    "        for mb in train_data_loader:\n",
    "            # ld = mb[0].to('cuda')\n",
    "            ld = mb[0]\n",
    "            af = self.ext_features(ld)\n",
    "            print(af.shape)\n",
    "            # ipdb.set_trace()\n",
    "            # pass\n",
    "            # for sample, label, task in mb:\n",
    "            # for sample in mb:\n",
    "            #     self.ext_features(sample)\n",
    "\n",
    "        # for sample in train_dataset:\n",
    "        # n_samples = len(train_dataset)\n",
    "        # for ix in range(n_samples):\n",
    "        # for ix in tqdm(range(n_samples)):\n",
    "            # ls, ll, lt = train_dataset[ix]\n",
    "            # self.ext_features(ls)\n",
    "            # self.model(ls)\n",
    "\n",
    "            # # ipdb.set_trace()\n",
    "            # jl.sample = ls.flatten().numpy()\n",
    "            # jl.label = ll\n",
    "            # jl.t = lt\n",
    "            # jl.eval(\"train!(art, sample, y=label)\")\n",
    "\n",
    "    def eval(self, experience):\n",
    "        eval_dataset = experience.dataset\n",
    "        t = experience.task_label\n",
    "        eval_data_loader = DataLoader(\n",
    "            # eval_dataset, num_workers=4, batch_size=128\n",
    "            eval_dataset, num_workers=4, batch_size=32\n",
    "        )\n",
    "        print(eval_data_loader)\n",
    "\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for mb in eval_data_loader:\n",
    "            for sample, label in mb:\n",
    "        # for mb in eval_data_loader:\n",
    "        # for sample in eval_dataset:\n",
    "                jl.sample = sample\n",
    "                jl.label = label\n",
    "                y_hat = jl.eval(\"classify(art, sample)\")\n",
    "                total += 1\n",
    "                if y_hat == label:\n",
    "                    correct += 1\n",
    "        \n",
    "        return correct/total\n",
    "        # j.samples = mb\n",
    "        # y_hats = j.eval(\"classify(art, samples)\")\n",
    "        # accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 GB\n"
     ]
    }
   ],
   "source": [
    "def print_allocated_memory():\n",
    "   print(\"{:.2f} GB\".format(torch.cuda.memory_allocated() / 1024 ** 3))\n",
    "\n",
    "print_allocated_memory()\n",
    "# torch.cuda.empty_cache()\n",
    "# print_allocated_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience  0\n",
      "12080\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart of experience \u001b[39m\u001b[39m\"\u001b[39m, experience\u001b[39m.\u001b[39mcurrent_experience)\n\u001b[0;32m     23\u001b[0m     \u001b[39m# print(experience.dataset)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     cl_strategy\u001b[39m.\u001b[39;49mtrain(experience)\n\u001b[0;32m     25\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining completed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39m#     print('Computing accuracy on the current test set')\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39m#     cl_strategy.eval(benchmark.test_stream[exp_id])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 68\u001b[0m, in \u001b[0;36mDDVFAStrategy.train\u001b[1;34m(self, experience)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m mb \u001b[39min\u001b[39;00m train_data_loader:\n\u001b[0;32m     66\u001b[0m     \u001b[39m# ld = mb[0].to('cuda')\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     ld \u001b[39m=\u001b[39m mb[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 68\u001b[0m     af \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mext_features(ld)\n\u001b[0;32m     69\u001b[0m     \u001b[39mprint\u001b[39m(af\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m, in \u001b[0;36mDDVFAStrategy.ext_features\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     44\u001b[0m prep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(img)\n\u001b[0;32m     45\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmod(prep)[\u001b[39m'\u001b[39m\u001b[39mlayer4\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 47\u001b[0m avg_features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39;49mmean(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mflatten()\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m     48\u001b[0m \u001b[39m# avg_features.flatten().detach().numpy()\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m avg_features\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitMNIST\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "from torchvision.transforms import Lambda\n",
    "\n",
    "trans = Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
    "# Benchmark creation\n",
    "benchmark = SplitMNIST(\n",
    "    n_experiences=5,\n",
    "    train_transform=trans,\n",
    "    eval_transform=trans,\n",
    ")\n",
    "# benchmark = SplitCIFAR10(n_experiences=5, return_task_id=True)\n",
    "\n",
    "# Create the Strategy Instance\n",
    "cl_strategy = DDVFAStrategy()\n",
    "\n",
    "# Training Loop\n",
    "print('Starting experiment...')\n",
    "\n",
    "for exp_id, experience in enumerate(benchmark.train_stream):\n",
    "    print(\"Start of experience \", experience.current_experience)\n",
    "\n",
    "    # print(experience.dataset)\n",
    "    cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "#     print('Computing accuracy on the current test set')\n",
    "#     cl_strategy.eval(benchmark.test_stream[exp_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\Sasha/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62aa0967b644ab8862741396b5b98b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow tie: 14.4%\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# img = read_image(\"test/assets/encode_jpeg/grace_hopper_517x606.jpg\")\n",
    "img = read_image(\"grace_hopper_517x606.jpg\")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = resnet50(weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Step 4: Use the model and print the predicted category\n",
    "prediction = model(batch).squeeze(0).softmax(0)\n",
    "class_id = prediction.argmax().item()\n",
    "score = prediction[class_id].item()\n",
    "category_name = weights.meta[\"categories\"][class_id]\n",
    "print(f\"{category_name}: {100 * score:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4': tensor([[[[1.2357e+00, 0.0000e+00, 1.1311e+00,  ..., 0.0000e+00,\n",
       "            1.3839e+00, 0.0000e+00],\n",
       "           [2.0044e+00, 0.0000e+00, 0.0000e+00,  ..., 6.7144e-01,\n",
       "            3.2935e+00, 0.0000e+00],\n",
       "           [1.1263e-01, 1.4880e+00, 0.0000e+00,  ..., 2.7060e+00,\n",
       "            2.0432e+00, 1.2406e+00],\n",
       "           ...,\n",
       "           [2.9631e+00, 2.8480e-01, 2.1829e+00,  ..., 0.0000e+00,\n",
       "            3.0080e+00, 1.5533e+00],\n",
       "           [1.4739e+00, 1.4666e+00, 5.5931e+00,  ..., 2.9052e+00,\n",
       "            1.6959e+00, 0.0000e+00],\n",
       "           [3.5369e-01, 2.3530e+00, 1.0774e+00,  ..., 8.4163e-01,\n",
       "            1.2362e+00, 0.0000e+00]],\n",
       " \n",
       "          [[5.0615e-02, 0.0000e+00, 0.0000e+00,  ..., 1.0539e+00,\n",
       "            5.5839e-03, 1.0187e+00],\n",
       "           [1.0786e+00, 5.3807e+00, 1.2345e+00,  ..., 1.2468e+00,\n",
       "            9.6508e-01, 0.0000e+00],\n",
       "           [1.2106e-01, 1.5680e+00, 0.0000e+00,  ..., 2.3732e+00,\n",
       "            1.1339e+00, 2.0850e+00],\n",
       "           ...,\n",
       "           [1.6436e+00, 3.8410e-01, 2.3757e+00,  ..., 3.2815e+00,\n",
       "            2.2893e+00, 0.0000e+00],\n",
       "           [1.5775e+00, 1.2433e+00, 4.8358e-01,  ..., 7.3616e-01,\n",
       "            0.0000e+00, 1.0862e+00],\n",
       "           [3.2912e+00, 1.9870e+00, 0.0000e+00,  ..., 7.9588e-01,\n",
       "            1.6117e+00, 2.8350e-02]],\n",
       " \n",
       "          [[0.0000e+00, 3.5302e-01, 3.5755e-01,  ..., 0.0000e+00,\n",
       "            1.4301e-01, 0.0000e+00],\n",
       "           [0.0000e+00, 1.9290e+00, 4.6196e+00,  ..., 1.1568e+00,\n",
       "            0.0000e+00, 1.1280e+00],\n",
       "           [6.7439e-02, 2.0279e-01, 1.5494e-01,  ..., 0.0000e+00,\n",
       "            1.3499e+00, 1.7283e+00],\n",
       "           ...,\n",
       "           [5.8753e-01, 0.0000e+00, 2.2033e+00,  ..., 1.0725e-01,\n",
       "            0.0000e+00, 8.9153e-01],\n",
       "           [3.4785e-01, 2.3415e+00, 2.6217e-01,  ..., 3.1734e-01,\n",
       "            8.4373e-03, 7.6205e-01],\n",
       "           [3.0575e-01, 1.3972e+00, 2.4445e-01,  ..., 3.9344e-01,\n",
       "            1.1713e+00, 1.2467e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[3.9016e-01, 0.0000e+00, 7.8848e-02,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.2291e-01, 8.1194e-01, 9.0050e-01,  ..., 1.0891e+00,\n",
       "            9.0494e-01, 1.2665e+00],\n",
       "           [1.0192e+00, 3.5529e-01, 7.6256e-01,  ..., 1.2674e+00,\n",
       "            6.2550e-01, 5.4717e-02],\n",
       "           ...,\n",
       "           [4.7796e-01, 1.1550e+00, 8.6687e-01,  ..., 1.2401e+00,\n",
       "            1.9625e+00, 1.1654e+00],\n",
       "           [0.0000e+00, 1.6288e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 7.7616e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6868e-01,\n",
       "            0.0000e+00, 1.6280e+00]],\n",
       " \n",
       "          [[7.9019e-01, 9.5732e-02, 1.2997e+00,  ..., 0.0000e+00,\n",
       "            1.4569e-01, 3.0774e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 1.0363e+00,  ..., 0.0000e+00,\n",
       "            4.3996e-01, 5.6732e-01],\n",
       "           [0.0000e+00, 1.2748e+00, 5.7480e-01,  ..., 8.6609e-03,\n",
       "            1.1365e+00, 2.1131e+00],\n",
       "           ...,\n",
       "           [4.7339e-02, 1.4199e+00, 8.9874e-01,  ..., 9.2332e-01,\n",
       "            1.8415e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 8.0456e-01, 5.2222e-01,  ..., 8.4397e-01,\n",
       "            1.7494e+00, 4.3277e+00],\n",
       "           [4.1910e-01, 1.2541e+00, 2.4667e-01,  ..., 5.6176e-01,\n",
       "            2.4485e+00, 3.7365e+00]],\n",
       " \n",
       "          [[0.0000e+00, 1.9668e+00, 8.7153e-01,  ..., 0.0000e+00,\n",
       "            1.0036e+00, 6.1013e-01],\n",
       "           [0.0000e+00, 1.4924e+00, 6.0192e-01,  ..., 1.6621e+00,\n",
       "            4.8042e-01, 2.3510e-01],\n",
       "           [5.0192e-01, 0.0000e+00, 2.3598e+00,  ..., 3.8418e-01,\n",
       "            5.9688e-01, 5.1103e-01],\n",
       "           ...,\n",
       "           [3.1662e+00, 2.3448e+00, 0.0000e+00,  ..., 9.1042e-01,\n",
       "            3.7283e+00, 3.9252e+00],\n",
       "           [7.1010e-01, 7.1879e-01, 4.4050e-01,  ..., 8.4786e-01,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [1.0495e+00, 1.5297e+00, 0.0000e+00,  ..., 3.2165e+00,\n",
       "            0.0000e+00, 1.0558e+00]]],\n",
       " \n",
       " \n",
       "         [[[0.0000e+00, 0.0000e+00, 1.9611e+00,  ..., 1.2262e+00,\n",
       "            3.6038e-01, 0.0000e+00],\n",
       "           [7.5945e-01, 0.0000e+00, 0.0000e+00,  ..., 1.6731e+00,\n",
       "            1.9104e+00, 6.8688e-01],\n",
       "           [1.0096e+00, 1.8925e+00, 8.3237e-01,  ..., 5.1766e-01,\n",
       "            2.1577e+00, 6.0290e-02],\n",
       "           ...,\n",
       "           [1.6722e-01, 0.0000e+00, 9.5413e-01,  ..., 0.0000e+00,\n",
       "            4.1292e+00, 1.0171e+00],\n",
       "           [2.7492e+00, 5.6739e-01, 2.2910e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 2.9029e-01],\n",
       "           [0.0000e+00, 0.0000e+00, 3.4126e-01,  ..., 1.8527e+00,\n",
       "            1.4923e-01, 1.2744e+00]],\n",
       " \n",
       "          [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2350e+00,\n",
       "            1.2188e+00, 2.6841e-01],\n",
       "           [0.0000e+00, 4.0672e-01, 2.2003e+00,  ..., 0.0000e+00,\n",
       "            4.6055e-01, 5.6640e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           ...,\n",
       "           [7.7277e-01, 1.8163e+00, 6.9168e-01,  ..., 3.8905e-01,\n",
       "            2.3216e+00, 5.4484e-01],\n",
       "           [6.4671e-01, 0.0000e+00, 2.3476e-01,  ..., 1.1128e+00,\n",
       "            3.1407e+00, 1.0179e+00],\n",
       "           [1.8831e+00, 1.9540e+00, 6.5742e-01,  ..., 3.8226e+00,\n",
       "            2.7193e-01, 1.8659e+00]],\n",
       " \n",
       "          [[1.3061e+00, 8.0998e-01, 4.3075e-01,  ..., 2.5189e+00,\n",
       "            7.1237e-01, 1.0030e+00],\n",
       "           [0.0000e+00, 5.1517e-01, 6.9241e-01,  ..., 1.4650e+00,\n",
       "            2.9804e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 2.1879e-01, 0.0000e+00,  ..., 7.3838e-01,\n",
       "            6.6247e-01, 1.1592e+00],\n",
       "           ...,\n",
       "           [0.0000e+00, 0.0000e+00, 3.1841e+00,  ..., 1.7897e+00,\n",
       "            0.0000e+00, 3.3355e+00],\n",
       "           [0.0000e+00, 1.0826e-01, 1.5217e+00,  ..., 3.9213e+00,\n",
       "            2.2456e+00, 0.0000e+00],\n",
       "           [0.0000e+00, 0.0000e+00, 1.9336e-01,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[2.8340e-01, 0.0000e+00, 1.6255e+00,  ..., 3.8972e-01,\n",
       "            6.9858e-01, 6.6479e-01],\n",
       "           [1.3480e+00, 2.0977e+00, 1.3461e+00,  ..., 1.8686e-01,\n",
       "            4.5005e-01, 4.7150e-01],\n",
       "           [0.0000e+00, 1.2273e+00, 0.0000e+00,  ..., 1.9156e+00,\n",
       "            2.8044e+00, 1.3953e+00],\n",
       "           ...,\n",
       "           [2.9884e-01, 1.1924e+00, 0.0000e+00,  ..., 3.5197e+00,\n",
       "            2.2135e+00, 4.8988e+00],\n",
       "           [0.0000e+00, 1.2034e+00, 0.0000e+00,  ..., 4.9351e+00,\n",
       "            1.0505e+00, 8.7417e-01],\n",
       "           [1.3331e+00, 1.0913e+00, 0.0000e+00,  ..., 1.4820e+00,\n",
       "            1.3899e+00, 1.1086e+00]],\n",
       " \n",
       "          [[1.4122e+00, 9.8026e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 0.0000e+00],\n",
       "           [4.2425e-01, 1.2803e+00, 2.6614e+00,  ..., 1.9060e+00,\n",
       "            1.4110e+00, 4.0951e-01],\n",
       "           [1.3136e+00, 1.2794e+00, 5.5088e-01,  ..., 2.9552e-01,\n",
       "            0.0000e+00, 1.8386e+00],\n",
       "           ...,\n",
       "           [3.2958e-01, 5.0137e-01, 0.0000e+00,  ..., 0.0000e+00,\n",
       "            0.0000e+00, 4.9273e-01],\n",
       "           [8.5784e-01, 0.0000e+00, 1.6256e+00,  ..., 2.0338e+00,\n",
       "            3.8430e+00, 1.7574e+00],\n",
       "           [3.1784e-01, 0.0000e+00, 5.0319e-01,  ..., 0.0000e+00,\n",
       "            2.7381e-01, 1.2946e+00]],\n",
       " \n",
       "          [[0.0000e+00, 4.9305e-01, 4.7554e-01,  ..., 6.6881e-01,\n",
       "            3.9385e-01, 3.0211e-01],\n",
       "           [6.9086e-01, 1.5418e+00, 0.0000e+00,  ..., 8.0481e-01,\n",
       "            5.7354e-01, 0.0000e+00],\n",
       "           [3.9543e-02, 2.5031e-02, 0.0000e+00,  ..., 1.8771e+00,\n",
       "            1.9157e+00, 1.2379e+00],\n",
       "           ...,\n",
       "           [1.5587e+00, 9.1079e-01, 8.9249e-01,  ..., 9.0184e-01,\n",
       "            2.2783e+00, 2.4392e+00],\n",
       "           [1.0870e-01, 1.2543e+00, 4.3404e+00,  ..., 8.6517e-02,\n",
       "            0.0000e+00, 3.4230e+00],\n",
       "           [1.0529e+00, 2.3527e+00, 0.0000e+00,  ..., 3.3381e+00,\n",
       "            1.7277e-01, 6.4551e-01]]]], grad_fn=<ReluBackward0>)}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.randn(2, 3, 224, 224)\n",
    "model(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example trains on Split CIFAR10 with Naive strategy.\n",
    "In this example each experience has a different task label.\n",
    "We use a multi-head model with a separate classifier for each task.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from avalanche.benchmarks.classic import SplitCIFAR10\n",
    "from avalanche.models import SimpleMLP, as_multitask\n",
    "from avalanche.training.supervised import Naive\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    # model\n",
    "    model = SimpleMLP(input_size=32 * 32 * 3, num_classes=10)\n",
    "    model = as_multitask(model, \"classifier\")\n",
    "\n",
    "    # CL Benchmark Creation\n",
    "    benchmark = SplitCIFAR10(n_experiences=5, return_task_id=True)\n",
    "    train_stream = benchmark.train_stream\n",
    "    test_stream = benchmark.test_stream\n",
    "\n",
    "    # Prepare for training & testing\n",
    "    optimizer = Adam(model.parameters(), lr=0.01)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Choose a CL strategy\n",
    "    strategy = Naive(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_mb_size=128,\n",
    "        train_epochs=3,\n",
    "        eval_mb_size=128,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # train and test loop\n",
    "    for train_task in train_stream:\n",
    "        strategy.train(train_task, num_workers=0)\n",
    "        strategy.eval(test_stream)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--cuda\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'targets_task_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mavalanche\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbenchmarks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m TaskBalancedDataLoader\n\u001b[0;32m      4\u001b[0m benchmark \u001b[39m=\u001b[39m SplitMNIST(\u001b[39m5\u001b[39m, return_task_id\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m dl \u001b[39m=\u001b[39m TaskBalancedDataLoader([exp\u001b[39m.\u001b[39;49mdataset \u001b[39mfor\u001b[39;49;00m exp \u001b[39min\u001b[39;49;00m benchmark\u001b[39m.\u001b[39;49mtrain_stream], batch_size\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m x, y, t \u001b[39min\u001b[39;00m dl:\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(t\u001b[39m.\u001b[39mtolist())\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\avalanche\\benchmarks\\utils\\data_loader.py:89\u001b[0m, in \u001b[0;36mTaskBalancedDataLoader.__init__\u001b[1;34m(self, data, oversample_small_tasks, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m# split data by task.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m task_datasets \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 89\u001b[0m \u001b[39mfor\u001b[39;00m task_label \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mtargets_task_labels\u001b[39m.\u001b[39muniques:\n\u001b[0;32m     90\u001b[0m     tidxs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mtargets_task_labels\u001b[39m.\u001b[39mval_to_idx[task_label]\n\u001b[0;32m     91\u001b[0m     tdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msubset(tidxs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'targets_task_labels'"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitMNIST\n",
    "# from avalanche.benchmarks.utils.data_loader import GroupBalancedDataLoader\n",
    "from avalanche.benchmarks.utils.data_loader import TaskBalancedDataLoader\n",
    "benchmark = SplitMNIST(5, return_task_id=True)\n",
    "\n",
    "dl = TaskBalancedDataLoader([exp.dataset for exp in benchmark.train_stream], batch_size=4)\n",
    "for x, y, t in dl:\n",
    "    print(t.tolist())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avalanche",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
