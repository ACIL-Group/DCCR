{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class MyStrategy():\n",
    "    \"\"\"My Basic Strategy\"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def train(self, experience):\n",
    "        # here you can implement your own training loop for each experience (i.e. \n",
    "        # batch or task).\n",
    "\n",
    "        train_dataset = experience.dataset\n",
    "        t = experience.task_label\n",
    "        train_data_loader = DataLoader(\n",
    "            train_dataset, num_workers=4, batch_size=128\n",
    "        )\n",
    "\n",
    "        for epoch in range(1):\n",
    "            for mb in train_data_loader:\n",
    "                # you magin here...\n",
    "                pass\n",
    "\n",
    "    def eval(self, experience):\n",
    "        # here you can implement your own eval loop for each experience (i.e. \n",
    "        # batch or task).\n",
    "\n",
    "        eval_dataset = experience.dataset\n",
    "        t = experience.task_label\n",
    "        eval_data_loader = DataLoader(\n",
    "            eval_dataset, num_workers=4, batch_size=128\n",
    "        )\n",
    "\n",
    "        # eval here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.benchmarks import SplitMNIST\n",
    "\n",
    "# Benchmark creation\n",
    "benchmark = SplitMNIST(n_experiences=5)\n",
    "\n",
    "# Model Creation\n",
    "model = SimpleMLP(num_classes=benchmark.n_classes)\n",
    "\n",
    "# Create the Strategy Instance (MyStrategy)\n",
    "cl_strategy = MyStrategy(\n",
    "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    CrossEntropyLoss())\n",
    "\n",
    "# Training Loop\n",
    "print('Starting experiment...')\n",
    "\n",
    "for exp_id, experience in enumerate(benchmark.train_stream):\n",
    "    print(\"Start of experience \", experience.current_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the current test set')\n",
    "    cl_strategy.eval(benchmark.test_stream[exp_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [5, 6]\n",
      "-- >> Start of training phase << --\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'EmptyTransformGroups.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCurrent Classes: \u001b[39m\u001b[39m\"\u001b[39m, experience\u001b[39m.\u001b[39mclasses_in_this_experience)\n\u001b[0;32m     56\u001b[0m \u001b[39m# train returns a dictionary which contains all the metric values\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m res \u001b[39m=\u001b[39m cl_strategy\u001b[39m.\u001b[39;49mtrain(experience, num_workers\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m     58\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining completed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mComputing accuracy on the whole test set\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:146\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m           experiences: Union[CLExperience,\n\u001b[0;32m    141\u001b[0m                              ExpSequence],\n\u001b[0;32m    142\u001b[0m           eval_streams: Optional[Sequence[Union[CLExperience,\n\u001b[0;32m    143\u001b[0m                                                 ExpSequence]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    144\u001b[0m           \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 146\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mtrain(experiences, eval_streams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_last_metrics()\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\avalanche\\training\\templates\\base.py:116\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperience \u001b[39min\u001b[39;00m experiences:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_training_exp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_exp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperience, eval_streams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training_exp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:264\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[1;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_epoch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training_epoch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\avalanche\\training\\templates\\update_type\\sgd_update.py:9\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_epoch\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m      4\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Training epoch.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[39m    :param kwargs:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m    :return:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmbatch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader:\n\u001b[0;32m     10\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_training:\n\u001b[0;32m     11\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\avalanche\\benchmarks\\utils\\data_loader.py:104\u001b[0m, in \u001b[0;36mTaskBalancedDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 104\u001b[0m     \u001b[39mfor\u001b[39;00m el \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dl\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m():\n\u001b[0;32m    105\u001b[0m         \u001b[39myield\u001b[39;00m el\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\avalanche\\benchmarks\\utils\\data_loader.py:204\u001b[0m, in \u001b[0;36mGroupBalancedDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m iter_dataloaders \u001b[39m=\u001b[39m []\n\u001b[0;32m    203\u001b[0m \u001b[39mfor\u001b[39;00m dl \u001b[39min\u001b[39;00m dataloaders:\n\u001b[1;32m--> 204\u001b[0m     iter_dataloaders\u001b[39m.\u001b[39mappend(\u001b[39miter\u001b[39;49m(dl))\n\u001b[0;32m    206\u001b[0m max_num_mbatches \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m([\u001b[39mlen\u001b[39m(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m dataloaders])\n\u001b[0;32m    207\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_num_mbatches):\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sasha\\Anaconda3\\envs\\avalanche\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't pickle local object 'EmptyTransformGroups.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics,\\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, StreamConfusionMatrix,\\\n",
    "    disk_usage_metrics, gpu_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training import Naive\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "benchmark = SplitMNIST(n_experiences=5)\n",
    "\n",
    "# MODEL CREATION\n",
    "model = SimpleMLP(num_classes=benchmark.n_classes)\n",
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns \n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger()\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open('log.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    StreamConfusionMatrix(num_classes=benchmark.n_classes, save_image=False),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, tb_logger]\n",
    ")\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE (NAIVE)\n",
    "cl_strategy = Naive(\n",
    "    model, SGD(model.parameters(), lr=0.001, momentum=0.9),\n",
    "    CrossEntropyLoss(), train_mb_size=500, train_epochs=1, eval_mb_size=100,\n",
    "    evaluator=eval_plugin)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, num_workers=4)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # eval also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream, num_workers=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- >> Start of training phase << --\n",
      "100%|██████████| 1875/1875 [00:16<00:00, 112.09it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.4871\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8621\n",
      "100%|██████████| 1875/1875 [00:14<00:00, 131.32it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2583\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9248\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:01<00:00, 161.52it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1834\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9487\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:01<00:00, 161.81it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 2.5171\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.1515\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:01<00:00, 162.44it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 2.6635\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.1543\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 1.7880\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.4182\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 1875/1875 [00:14<00:00, 130.68it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.4060\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8761\n",
      "100%|██████████| 1875/1875 [00:14<00:00, 130.64it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2275\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9331\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:01<00:00, 162.11it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.2334\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9255\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:01<00:00, 161.81it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.1600\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9535\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:01<00:00, 160.86it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 2.8864\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.1277\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 1.0933\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.6689\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 1875/1875 [00:14<00:00, 131.86it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.3844\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8832\n",
      "100%|██████████| 1875/1875 [00:14<00:00, 132.19it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.2099\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9394\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:02<00:00, 153.78it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.2728\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9157\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:02<00:00, 154.96it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.1766\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9489\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 313/313 [00:02<00:00, 154.61it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 0.1448\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.9569\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 0.1981\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.9405\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "\n",
    "from avalanche.benchmarks.classic import PermutedMNIST\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training import Naive\n",
    "\n",
    "\n",
    "# Config\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model\n",
    "model = SimpleMLP(num_classes=10)\n",
    "\n",
    "# CL Benchmark Creation\n",
    "perm_mnist = PermutedMNIST(n_experiences=3)\n",
    "train_stream = perm_mnist.train_stream\n",
    "test_stream = perm_mnist.test_stream\n",
    "\n",
    "# Prepare for training & testing\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Continual learning strategy\n",
    "cl_strategy = Naive(\n",
    "    model, optimizer, criterion, train_mb_size=32, train_epochs=2,\n",
    "    eval_mb_size=32, device=device)\n",
    "\n",
    "# train and test loop over the stream of experiences\n",
    "results = []\n",
    "for train_exp in train_stream:\n",
    "    cl_strategy.train(train_exp)\n",
    "    results.append(cl_strategy.eval(test_stream))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avalanche",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
